# Basic Operation
> Contents of the basic operation in deep learning  

![pic](Img/summary.jpg)

## Convolution
- [A Guide to Convolution Arithmetic for Deep Learning](https://arxiv.org/abs/1603.07285)  

## Normalization
- [Batch Normalization](https://arxiv.org/abs/1502.03167)
- [Group Normalization](https://arxiv.org/abs/1803.08494)
- [Instance Normalization](https://arxiv.org/abs/1607.08022)
- [Layer Normalizartion](https://arxiv.org/pdf/1607.06450.pdf)
- [Adaptive Instance Normalization](https://arxiv.org/pdf/1703.06868.pdf)

## Activation  
- [Rectified Linear Unit](http://proceedings.mlr.press/v15/glorot11a.html)
- [Leaky Rectified Linear Unit](https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf)
- [Parametric Rectified Linear Unit](https://arxiv.org/abs/1502.01852)  

## Dropout
- [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://pdfs.semanticscholar.org/6c8b/30f63f265c32e26d999aa1fef5286b8308ad.pdf?_ga=2.6613809.2000749700.1589247527-1305559627.1585206618)

## Attention Mechanism
- [Spatial Attention](https://arxiv.org/pdf/1502.03044.pdf)
- [Channel Attention](http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/1287.pdf)
- [Self Attention](https://arxiv.org/pdf/1805.08318.pdf)

## Neural Network Module
- [Non-local Block](https://arxiv.org/abs/1711.07971v1)
- [Point Rend](https://arxiv.org/pdf/1912.08193.pdf)
